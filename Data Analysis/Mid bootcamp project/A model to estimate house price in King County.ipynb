{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ccc30e",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MODEL-1\" data-toc-modified-id=\"MODEL-1-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MODEL 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-1-using-Linear-Regression\" data-toc-modified-id=\"Model-1-using-Linear-Regression-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Model 1 using Linear Regression</a></span></li></ul></li><li><span><a href=\"#Data-Cleaning-&amp;-Standarization\" data-toc-modified-id=\"Data-Cleaning-&amp;-Standarization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Cleaning &amp; Standarization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importing-data\" data-toc-modified-id=\"Importing-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Importing data</a></span></li><li><span><a href=\"#Checking-Null-Values\" data-toc-modified-id=\"Checking-Null-Values-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Checking Null Values</a></span></li><li><span><a href=\"#Checking-for-duplicated-Values\" data-toc-modified-id=\"Checking-for-duplicated-Values-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Checking for duplicated Values</a></span></li></ul></li><li><span><a href=\"#Pre-processing\" data-toc-modified-id=\"Pre-processing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pre-processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Checking-data-types\" data-toc-modified-id=\"Checking-data-types-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Checking data types</a></span></li><li><span><a href=\"#Checking-data-shapes\" data-toc-modified-id=\"Checking-data-shapes-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Checking data shapes</a></span></li><li><span><a href=\"#Check-useless-columns\" data-toc-modified-id=\"Check-useless-columns-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Check useless columns</a></span></li><li><span><a href=\"#Dealing-with-the-categorical-variables\" data-toc-modified-id=\"Dealing-with-the-categorical-variables-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Dealing with the categorical variables</a></span></li></ul></li><li><span><a href=\"#Testing-the-model\" data-toc-modified-id=\"Testing-the-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Testing the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Train test split</a></span></li><li><span><a href=\"#Linear-regression-model\" data-toc-modified-id=\"Linear-regression-model-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Linear regression model</a></span></li><li><span><a href=\"#Model-Validation\" data-toc-modified-id=\"Model-Validation-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Model Validation</a></span></li><li><span><a href=\"#Testing-other-models\" data-toc-modified-id=\"Testing-other-models-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Testing other models</a></span></li></ul></li><li><span><a href=\"#Scale-all-of-X\" data-toc-modified-id=\"Scale-all-of-X-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Scale all of X</a></span><ul class=\"toc-item\"><li><span><a href=\"#Min-Max-Scaler\" data-toc-modified-id=\"Min-Max-Scaler-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Min Max Scaler</a></span></li><li><span><a href=\"#StandardScaler\" data-toc-modified-id=\"StandardScaler-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>StandardScaler</a></span></li><li><span><a href=\"#RobustScaler\" data-toc-modified-id=\"RobustScaler-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>RobustScaler</a></span></li></ul></li><li><span><a href=\"#MODEL-2---Setting-distance-to-center-as-dummie\" data-toc-modified-id=\"MODEL-2---Setting-distance-to-center-as-dummie-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>MODEL 2 - Setting distance to center as dummie</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-new-dataframe\" data-toc-modified-id=\"Import-new-dataframe-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Import new dataframe</a></span></li><li><span><a href=\"#Same-cleaning-steps-as-the-previous-model\" data-toc-modified-id=\"Same-cleaning-steps-as-the-previous-model-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Same cleaning steps as the previous model</a></span></li><li><span><a href=\"#Training-and-testing-the-new-model\" data-toc-modified-id=\"Training-and-testing-the-new-model-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Training and testing the new model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e64c29",
   "metadata": {},
   "source": [
    "# MODEL 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edd99f",
   "metadata": {},
   "source": [
    "- Setting Zipcode as dummie \n",
    "- Adding a new year column (either the construction date or the reform date if reformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed87db",
   "metadata": {},
   "source": [
    "## Model 1 using Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408e9ae",
   "metadata": {},
   "source": [
    "# Data Cleaning & Standarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447ed05",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),('pca', \n",
    "                  PCA(n_components=4)),('slr', LinearRegression())])\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('regression_data_decade.xls')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953503fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84965c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a8f1c",
   "metadata": {},
   "source": [
    "## Checking Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff4a26",
   "metadata": {},
   "source": [
    "Our first step was trying to find the null values. The dataset doesn't have any null values  so we don't have to deal with them. However, it is important to think about the reason why these nulls don't exist, since this can introduce some kind of bias in the data. We have to observe if data may have been duplicated to avoid nulls, if random values have been incorporated..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b396666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ebbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.notnull().sum() # We haven't detected null values in any columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fecdfb",
   "metadata": {},
   "source": [
    "## Checking for duplicated Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228365ee",
   "metadata": {},
   "source": [
    "Our approach to finding duplicated values was first of all checking the reason why a same id was repeated. Due to the fact that we only had data from 2014 and 2015, probably the only reason why a house may be repeated it's because it was sold two times in this period and, therefore, two different prices (but the independent variables remained the same.) That's why we decided to only keep the last date transaction info since it's the one that recaps better the actual price of that home. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f586423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.id.duplicated().sum() #checking how many duplicated ids(Houses) are there in the Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c693488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum() #Checking if there are duplicated rows. There are not any identical rows so the duplicated Ids may have some difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17822f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.id.duplicated(),:].sort_values(by=['id']) # We check all the duplicated Ids in the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecce793",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "df[df.duplicated(['id'], keep=False)].sort_values(by=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae5581",
   "metadata": {},
   "source": [
    "We check for all the ids that are duplicated in all the columns except the price and drop the oldest date of those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af72350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dupl=df[df.duplicated(['id','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view','condition','grade', 'sqft_above','sqft_basement','yr_built','yr_renovated','zipcode','lat','long','sqft_living15', 'sqft_lot15'], keep=False)].sort_values(by=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ac0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates( subset=['id','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view','condition','grade', 'sqft_above','sqft_basement','yr_built','yr_renovated','zipcode','lat','long','sqft_living15', 'sqft_lot15'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefb83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c94966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a254d",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d6933",
   "metadata": {},
   "source": [
    "## Checking data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25876959",
   "metadata": {},
   "source": [
    "We first thought of converting  bathrooms to an integer but since it can also take decimals values we decided to leave it as a float. However, we did convert floors into an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['floors']=df['floors'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf2c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69130ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e776af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['id'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b1578",
   "metadata": {},
   "source": [
    "## Checking data shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52c9c5",
   "metadata": {},
   "source": [
    "We first plot all the graphs to try to detect clear outliers. At first sight, most of the numerical columns(sqft_living, sqft_living15, sqft_lot, sqft_lot15, sqft_above, sqft_basement) seem to have some outliers. For the sqft_living variables, since it behaves as a kind of normal distribution, we decided to drop the outliers that were outside the range of 3 standard deviations. For the sqft_lot, we drop all the values that were greater than 8-hundred thousand. For both variables we tried to make a log transformation to avoid the skewness of the model but it didn't contribute to the success of the model. We also see a clear correlation between sqft_living and sqft_above so we may only include one of the two in the model. Regarding the categorical variables such as bedrooms, we'll deal with non-sense outliers such as 33 and 11 bedrooms(not consistent with the rest of the attributes of the house). For the 33 bedrooms house, we'll treat it as a typo and interpret it as 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(15,15),bins=20,layout=(6,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594330ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(['bedrooms'], figsize=(13,11),bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f984f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bedrooms'] = df['bedrooms'].replace(33,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[df['bedrooms']==11].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(['bedrooms'], figsize=(13,11),bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(['floors'], figsize=(13,11),bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8862cd0",
   "metadata": {},
   "source": [
    "## Check useless columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c87067",
   "metadata": {},
   "source": [
    "For checking which columns should we add to our model, we run both the correlation matrix and the scatter_matrix so that we could check for multicollinearity, which were the variables more related to the price...\n",
    "After observing the scatter_matrix, and observing that the sqft_living behaved as a kind of normal distribution we decided to deal with it's outliers by droping the values away from it's mean and 3 std. deviations. We introduced to the model the sqft_living and sqft_basement variables. The reason for the first (sqft_living) is that is the variable more correlated with the target and it's very correlated with another numeric variable (sqft_above) that we drop to avoid multicollinearity. Regarding sqft_basement, is not as correlated to sqft_living so we'll live it in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915176fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df=df.drop(['date','long','yr_built','zipcode','yr_renovated','grade','condition','view','waterfront'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Matrix\n",
    "corre_matrix=corr_df.corr()\n",
    "corre_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2743c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "mask=np.zeros_like(corre_matrix)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "fig,ax = plt.subplots(figsize=(14,10))\n",
    "ax=sns.heatmap(corre_matrix, mask=mask, center=0, cmap=sns.diverging_palette(220, 20, as_cmap=True), annot=True,);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "df=df[(np.abs(stats.zscore(df['sqft_living'])) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb3546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[df['sqft_lot']>900000].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96820c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num=df[['sqft_living','bedrooms','bathrooms','floors', 'sqft_basement','lat']]\n",
    "X_num_scatter_matrix=df[['sqft_above','sqft_living','sqft_basement','sqft_lot' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(X_num_scatter_matrix,alpha=0.2, figsize=(20,12), diagonal='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ee251",
   "metadata": {},
   "source": [
    "## Dealing with the categorical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a03c76",
   "metadata": {},
   "source": [
    "As we commented before, we added a categorical variable (Decade build) that showed the date whether it was built or it was renovated. We afterwards group them by decades. Another step, it was dummifying all our categorical variables. This was one of the main limitations of the model. When dummifying the zipcode, we created a lot of little subsamples that may not have many observations and therefore drive us to error. This problem gets bigger when we drop some outliers that may reduce even more the size of our subsamples. However, we didn't see a way of finding a correct beta(coeficient) for this variables without dummyfying them. We also executed some chisquared test to check for correlations between categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af372983",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_crosstab=pd.crosstab(df['condition'],df['grade'], margins=False)\n",
    "feat_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d365aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency \n",
    "chi2_contingency(feat_crosstab, correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_crosstab=pd.crosstab(df['view'],df['grade'], margins=False)\n",
    "feat_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ab342",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_contingency(feat_crosstab, correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat1=df[['grade', 'Decade Build', 'condition','grade','view']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies1=pd.get_dummies(X_cat1, drop_first=True)\n",
    "X_dummies1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e043d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat2=df[['zipcode']].astype(str)\n",
    "X_cat2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a937e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies2=pd.get_dummies(X_cat2, drop_first=True)\n",
    "X_dummies2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies=pd.concat((X_dummies1,X_dummies2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef05c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final=pd.concat((X_num,X_dummies),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccac1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777d46f",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbbd01",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9d970",
   "metadata": {},
   "source": [
    "Once all the engineering and the pre-processing is finished we, as usual, run and test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07489acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824999d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_final,y,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0370e0",
   "metadata": {},
   "source": [
    "## Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression() #configure model\n",
    "model=lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65096d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=lm.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4a6a1",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d615e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62378b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(y_test, preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse=np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb1ff6",
   "metadata": {},
   "source": [
    "As we have already mentioned, the problem with our model was that it had too many variables. That is why, in order to try to reduce this problem, we have tried some kind of feature selection model that allows us to drop the variables that contribute the least to the model. By doing this, we may save some time and computing cost. If from the 95 original variables we shrink to 45(the most important for the model) we only lose 0.1 od the R2 score. The code used was the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING-ADAPTING\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "sfs1 = SFS(linear_model.LinearRegression(),\n",
    "            k_features=90,\n",
    "            forward=True,\n",
    "            scoring='r2',\n",
    "            cv=3)\n",
    "sfs1 = sfs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a75f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.subsets_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2c51f",
   "metadata": {},
   "source": [
    "## Testing other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae5992",
   "metadata": {},
   "source": [
    "In order to try to improve the performance of our model, we have tried to use other models that could be better adjusted to the characteristics of the database we were working with. The models we tried were ('RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model', 'tree_Regressor'). After testing these models, both the RandomForestRegressor and GradientBoostingRegressor, are slightly better than the LinearRegressionModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11e596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "classifiers = ['RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model', 'tree_Regressor']\n",
    "models = [\n",
    "          RandomForestRegressor(n_estimators=200, random_state=0),\n",
    "          KNeighborsRegressor(),\n",
    "          GradientBoostingRegressor(random_state=0), linear_model.LinearRegression(),tree.DecisionTreeRegressor()\n",
    "         ]\n",
    "for i in models:\n",
    "    model = i\n",
    "    model.fit(X_train,y_train)\n",
    "    preds=model.predict(X_test)\n",
    "    print(model,'accuracy:',r2_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26a30e",
   "metadata": {},
   "source": [
    "# Scale all of X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de620fa",
   "metadata": {},
   "source": [
    "Another of the attempts to improve our model has been to scale the numerical data. For it, we used the maxmin_scaler, the st_scaler and the rob_scaler. However, we can't appreciaate a remarkable difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxmin_scaler (X):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    X_scaled = MinMaxScaler().fit(X).transform(X)\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "def st_scaler (X):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    X_scaled_st = StandardScaler().fit(X).transform(X)\n",
    "    return X_scaled_st\n",
    "\n",
    "def rob_scaler (X):\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    X_scaled_rob = RobustScaler().fit(X).transform(X)\n",
    "    return X_scaled_rob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901fd806",
   "metadata": {},
   "source": [
    "## Min Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled=maxmin_scaler(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584942e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler=MinMaxScaler()\n",
    "#X_scaled=scaler.fit_transform(X_num)\n",
    "#X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43703611",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df=pd.DataFrame(X_scaled, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bb368",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656037b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_concat=pd.concat((X_scaled_df,X_dummies),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f9c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_concat.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f13cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df6989",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled_concat,y,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression() #configure model\n",
    "model=lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=lm.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd989a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b946183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "classifiers = ['RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model']\n",
    "models = [\n",
    "          RandomForestRegressor(n_estimators=200, random_state=0),\n",
    "          KNeighborsRegressor(),\n",
    "          GradientBoostingRegressor(random_state=0), linear_model.LinearRegression(),\n",
    "         ]\n",
    "for i in models:\n",
    "    model = i\n",
    "    model.fit(X_train,y_train)\n",
    "    preds=model.predict(X_test)\n",
    "    print(model,'accuracy:',r2_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5065897",
   "metadata": {},
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled=st_scaler(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b3a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df=pd.DataFrame(X_scaled, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_concat=pd.concat((X_scaled_df,X_dummies),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a075cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled_concat,y,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd98ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression() #configure model\n",
    "model=lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=lm.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1646c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "classifiers = ['RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model']\n",
    "models = [\n",
    "          RandomForestRegressor(n_estimators=200, random_state=0),\n",
    "          KNeighborsRegressor(),\n",
    "          GradientBoostingRegressor(random_state=0), linear_model.LinearRegression(),\n",
    "         ]\n",
    "for i in models:\n",
    "    model = i\n",
    "    model.fit(X_train,y_train)\n",
    "    preds=model.predict(X_test)\n",
    "    print(model,'accuracy:',r2_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262f64f",
   "metadata": {},
   "source": [
    "## RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled=rob_scaler(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df=pd.DataFrame(X_scaled, columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98136d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_concat=pd.concat((X_scaled_df,X_dummies),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled_concat,y,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression() #configure model\n",
    "model=lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=lm.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3eeae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "classifiers = ['RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model']\n",
    "models = [\n",
    "          RandomForestRegressor(n_estimators=200, random_state=0),\n",
    "          KNeighborsRegressor(),\n",
    "          GradientBoostingRegressor(random_state=0), linear_model.LinearRegression(),\n",
    "         ]\n",
    "for i in models:\n",
    "    model = i\n",
    "    model.fit(X_train,y_train)\n",
    "    preds=model.predict(X_test)\n",
    "    print(model,'accuracy:',r2_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85a482",
   "metadata": {},
   "source": [
    "# MODEL 2 - Setting distance to center as dummie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae758a",
   "metadata": {},
   "source": [
    "In our second model, we tried to think a way of improving the score we obtained on our previous models. As we commented before, one of the main issues in our previous model was dealing with some many subsamples due to the dummification of the zipcode variables. We therefore, tried to reduce the number of subsamples by some kind of aggrupation that had more observations for each subsample. That's why we decided to divide the zipcodes in 5 groups depending how far away were they from the most expensive area (best place to live). It appeared to be a clear pattern that the distance to this point would mean less value of the house (less services, more distance to business area...). That's why we created the distance_to_center column. For the rest of the model, we just follow the same steps as before. However, probably due to the fact that our election of zipcodes group were arbitrary, we didn't manage to find a better performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b262d9e",
   "metadata": {},
   "source": [
    "## Import new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_excel('regression_data_distance_center.xls')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d340427",
   "metadata": {},
   "source": [
    "## Same cleaning steps as the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca29231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(df3.loc[df3['bedrooms']==11].index, inplace=True)\n",
    "df3['bedrooms'] = df3['bedrooms'].replace(33,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a42203",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(df3.loc[df3['bedrooms']==11].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce7035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3[(np.abs(stats.zscore(df3['sqft_living'])) < 3)]\n",
    "df3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(df3.loc[df3['sqft_lot']>900000].index, inplace=True)\n",
    "df3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop_duplicates(subset=['id','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view','condition','grade', 'sqft_above','sqft_basement','yr_built','yr_renovated','zipcode','lat','long','sqft_living15', 'sqft_lot15'],keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ace619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['bathrooms']=df3['bathrooms'].astype(int) \n",
    "df3['floors']=df3['floors'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3545241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop(['id'],axis=1)\n",
    "df3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b615227",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat3=df3[['grade', 'Decade Build', 'condition','distance_to_center']].astype(str)\n",
    "X_cat3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummies10=pd.get_dummies(X_cat3, drop_first=True)\n",
    "X_dummies10.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num2=df3[['sqft_living','bedrooms','bathrooms','floors', 'sqft_basement']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d035ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88956aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final2=pd.concat((X_num2,X_dummies10),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ef133",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8301e5",
   "metadata": {},
   "source": [
    "## Training and testing the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f1c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = df3['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73644a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_final2,y2,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13fdd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=linear_model.LinearRegression() #configure model\n",
    "model=lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe239f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2=lm.predict(X_test)\n",
    "preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc1dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7265146",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(y_test, preds2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddcc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse=np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf848e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "classifiers = ['RandomForestRegressor', 'KNeighborsRegressor','GradientBoostingRegressor', 'linear_model']\n",
    "models = [\n",
    "          RandomForestRegressor(n_estimators=200, random_state=0),\n",
    "          KNeighborsRegressor(),\n",
    "          GradientBoostingRegressor(random_state=0), linear_model.LinearRegression(),\n",
    "         ]\n",
    "for i in models:\n",
    "    model = i\n",
    "    model.fit(X_train,y_train)\n",
    "    preds=model.predict(X_test)\n",
    "    print(model,'accuracy:',r2_score(y_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING-ADAPTING\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "sfs1 = SFS(linear_model.LinearRegression(),\n",
    "            k_features=23,\n",
    "            forward=True,\n",
    "            scoring='r2',\n",
    "            cv=3).fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.subsets_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
